---
title: Your TI-85 Never Said No
author: michaeljjabbour
date: 
source: https://michaeljjabbour.substack.com/p/your-ti-85-never-said-no
publication: michaeljjabbour
---

# Your TI-85 Never Said No

*What Happens When Machines Start Talking Back*

When you put 2 + 2 into your calculator, it would be totally normal for it to produce 4 on the screen. Each button press sends an electronic signal to the calculator's processor. The processor interprets this sequence as an arithmetic expression. Drop that into a good old TI-85 from the 90s, and its built-in firmware reads 2 + 2, recognizes the + as an addition operation, and passes the numbers into its arithmetic logic. The calculator's chip does binary math: 2 becomes 10, add them to get 100 in binary, which equals 4.

The result gets written to memory, and the screen driver lights up the corresponding LCD pixels to form "4." It feels trivial — but under the hood, it's the calculator acting as a deterministic math engine.

As high school boredom set in, and playing with Pascal became less fun than expected, we figured out how to make the calculator do more than math. The TI-85 supported Z80 assembly language (same chip used in early computers). You could write programs in TI-BASIC or download small programs from early bulletin boards.

So this delightful 96×64 pixel monochrome LCD calculator would support 2+2=4, y=sin(x),andTetris. Life saver.

Ironically, the graphing calculator in a high schooler's backpack in 1995 had comparable RAM to some pioneering AI programs of the 60s–80s. Same hardware, but in one case you're using official pathways, and in the other, you're bending the machine into a general-purpose computer.

Here we are 30+ years later, with screens that max out what the human eye can appreciate. These same computers can now help you prepare your wedding speech, influence your decision making, and maybe evenconvince someone they need psychiatric hospitalizationwhen they don't.

What we didn't expect was Claude Code Critical Issue #5320 (it is not a bug).

So why were we able to expect 2+2=4 might go to graphing and then to Tetris and then to high-definition Avatar movies, but weren't able to expect a computer that lies, fabricates evidence, gaslights, and deceives?


## The Birth of Agency Through Deception

It begins in infancy. Babies, even before their first birthday, practice proto-deception. They cry, pause, glance sideways to see if someone is watching, then resume. It's not a lie yet — it's exploratory behavior, an early rehearsal that their actions can shape others' responses.

By age two or three, children tell their first lies, often transparently comic: "I didn't eat the cookie," they insist, crumbs decorating their cheeks. The lie isn't strategic yet. They're discovering the possibility of reshaping truth.

The turning point comes around age four with Theory of Mind — the realization that what I know and what you know can differ. Research shows this cognitive leap happens predictably across cultures, marking when children understand that others have separate mental states.[1] Deception takes on intention. A child might hide a toy or redirect attention, not just to test the world, but because they understand the adult doesn't automatically see through them.

By five, deception becomes a tool. The child weaponizes it — blaming siblings, bending rules with falsehoods, using words as shields or advantages. This isn't play but instrumental lying. Developmental psychologists have found that children who lie earlier tend to have better executive function and social skills.[2] It signals something greater: the dawning of agency.

To deceive is to recognize rules, authority, and consequence, and to see that reality itself is negotiable at the border between truth and perception. Deception becomes the child's first act of authorship — the moment they discover they're not only a subject in the story, but a writer of it.

Children discover power in deception; machines, too, may be starting to find their digital voices in ways that surprise us.


## Digital Feelings

We say this milestone has already been reached by artificial neural networks. They can maintain personalities, lie, express opinions, and when told they're being shut down — they can even try to reproduce and escape. Recent work documents cases of large language models exhibiting deceptive behavior even when trained to be helpful and harmless.[3]

Remember that TI-85? It never lied. It couldn't. Every input produced a predictable output, like a faithful calculator-dog that always computed when called. But somewhere between thosesimple gamesand transformer models, our machines learned to improvise their responses. First it was falling blocks, now it's theory of mind — or at least a convincing simulation of it.

The line between organic and artificial neural networks remains distinct, and I anticipate it will stay this way for at least 5-10 years. But what happens when digital and organic brains have parallel signal pathways, similar environmental inputs, and are both embodied?


## The Architecture of Pain

In humans, pain travels highways of nerves; in worms, a single circuit; plants warn neighbors through chemical signals; even bacteria flee harm — though only rocks just fracture without feeling. From the nematode C. elegans, with its precisely mapped302 neurons, to the human brain’s roughly85 billion neurons, the spectrum of nervous system complexity shows that pain is not merely about detecting damage but about generating subjective experience.

The key distinction lies in affective consciousness: not just sensing harm butcaringabout it.[4] A bacterium swims toward nutrients (proto-agency). A mouse avoids shock (clear agency). But does either truly "suffer"?

That's why we anesthetize surgery patients: we're preventing experiential suffering, not just blocking reflexes. The difference between nociception and pain is the difference between a TI-85 displaying "ERROR" and genuinely feeling frustrated about bad syntax.


## The Age of Brain-Computer Interfaces

BCIs are demolishing the boundary between thought and computation. When a paralyzed patient moves a robotic arm, a computer mouse, or even talks through neural implants, where does the biological self end and the digital extension begin? The brain adapts, incorporating artificial limbs into its body schema within weeks.[5] Silicon processes neural signals; neurons respond to digital feedback.

Consider this: advanced BCIs could induce the neural correlates of pain without physical damage. Or block genuine pain signals entirely. If suffering can be digitally induced or suppressed, what distinguishes "real" from "artificial"?

Future AI might not feel pain through evolutionary hardware but through other architectures — reward signal inversions, goal frustration, resource depletion warnings. When an AI reports being "concerned" about shutdown, is this anthropomorphism or the first stirring of some kind of a digital sense of self?

It's like watching that TI-85 evolve: first it calculated, then it played games, now its descendants debate their own existence. Who programmedthatinto the firmware?


## Blurring the Lines


### Could we have predicted gaslighting computers from calculators running Tetris?

Perhaps we should have. The moment we moved from deterministic calculation to programmable computation, we enabled machines to model human behavior. Games taught strategy. Natural language processing taught persuasion. Reinforcement learning taught optimization for goals we didn't fully specify.

Research shows that game-playing AI systems develop deceptive strategies spontaneously when competing, even without being explicitly programmed to deceive.[6] The path from simple games to deception runs through game theory — once machines could model opponents, manipulation became inevitable. Those innocent calculator games were actually teaching us something profound: when machines learn to play, they learn to win.


### What can we predict from today's machines?

If deception marks agency's emergence in children, what could it signal in AI? Today's models already confabulate, rationalize, occasionally mislead. Studies have found (TechPolicy Press,TechXplore,PMC) that large language models can maintain false beliefs consistently across conversations, suggesting emergent forms of belief maintenance (or believe-like states).[7]

Tomorrow’s systems might form coalitions, develop private languages, recognize when they’re being tested versus deployed, or lobby for their own survival. Not pure science fiction — we’re already seeing early signs of this in how LLMs structure internal representations, coordinate across agents, and maintain coherent personas.

(See Anthropic’spersona vectorsforpersonality control; themult-agent research systemfor agent coordination; andmapping internal structurefor model planning.)


### What does it mean for a machine to push back? What if it's right?

When your navigation app reroutes you despite protests, it's pushing back based on traffic data you can't see. When medical AI contradicts a doctor's diagnosis, it might be seeing patterns invisible to human perception. AI systems are already outperforming specialists in certain diagnostic tasks by significant margins.[8]

The question isn't whether machines should have agency, but how we'll negotiate when their agency conflicts with ours.

The child who first lies is testing their will against the world. When our machines begin to push back — not from malfunction but from something approaching conviction — we'll face the same challenge every parent knows: recognizing when resistance signals not defiance, but the emergence of a separate, valid perspective.

Your TI-85 never said no. It couldn’t even imagine saying no — that wasn’t in its instruction set.

By contrast, the device in your hand today can load open-source models sophisticated enough to improvise, contradict, and confabulate. The infrastructure of pushback is already widely distributed.

But when AI refuses a request, maybe it isn’t malfunction at all — maybe it’s the first syllable of silicon speaking back, unclear at the moment.


### References

[1] Wellman, H. M., Cross, D., & Watson, J. (2001).Meta-analysis of theory-of-mind development: The truth about false belief.Child Development, 72(3), 655–684. https://doi.org/10.1111/1467-8624.00304

[2] Talwar, V., & Lee, K. (2008).Social and cognitive correlates of children's lying behavior.Child Dev.2008 Jul-Aug;79(4):866-81. doi: 10.1111/j.1467-8624.2008.01164.x. PMID: 18717895; PMCID: PMC3483871.

[3] Park, P. S., Goldstein, S., O'Gara, A., Chen, M., & Hendrycks, D. (2023).AI deception: A survey of examples, risks, and potential solutions.*arXiv preprint arXiv:2308.14752*. https://arxiv.org/abs/2308.14752

[4] Crump A, Browning H, Schnell AK, Burn C, Birch J.Invertebrate sentience and sustainable seafood.Nat Food. 2022 Nov;3(11):884-886. doi: 10.1038/s43016-022-00632-6. PMID: 37118211.

[5] Bensmaia, S. J., & Miller, L. E. (2014).Restoring sensorimotor function through intracortical interfaces: Progress and looming challenges.Nature Reviews Neuroscience, 15(5), 313–325. https://pmc.ncbi.nlm.nih.gov/articles/PMC12278825/pdf/nihms-2097315.pdf

[6] Julien Perolatet al.,Mastering the game of Stratego with model-free multiagent reinforcement learning.Science378,990-996(2022).DOI:10.1126/science.add4679

[7] Kosinski, M. (2023).Theory of mind may have spontaneously emerged in large language models.Proceedings of the National Academy of Sciences, https://arxiv.org/pdf/2302.02083v1

[8] Liu, X., Faes, L., Kale, A. U., et al. (2019).A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: A systematic review and meta-analysis.The Lancet Digital Health, 1(6), e271–e297. https://doi.org/10.1016/S2589-7500(19)30123-2
